{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scripts.simulation_imports import *\n",
    "\n",
    "from openai import OpenAI\n",
    "from ast import literal_eval\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path.home() / Path(os.environ.get(\"RSYS_DATA\", \"rsys_data/rsys_2025\"))\n",
    "gen_slates_dir = DATA_PATH / \"gen_slates\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "feather_file_path_wp= gen_slates_dir / \"wp_llm_slates.feather\"\n",
    "df=pd.read_feather(feather_file_path_wp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path.home() / Path(os.environ.get(\"DATA_PATH\"))\n",
    "dataset_interaction_path = DATA_PATH / Path(\"MINDlarge_train/test_50.feather\")\n",
    "interaction_data = pd.read_feather(dataset_interaction_path)\n",
    "dataset_path =DATA_PATH / Path(\"MINDlarge_train/test_50.feather\")\n",
    "category_data = pd.read_feather(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path.home() / Path(os.environ.get(\"DATA_PATH\"))\n",
    "news_df = pd.read_feather(\n",
    "            base_path / Path(\"MINDlarge_train/news_glove_embed_50.feather\")\n",
    "        )\n",
    "embedding_dict = dict(zip(news_df[\"itemId\"], news_df[\"embedding\"]))\n",
    "embedding_lookup = {tuple(item_embedding): item_id for item_id, item_embedding in embedding_dict.items() if item_embedding is not None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wolpertinger+LLM Slates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve item IDs for candidate_docs\n",
    "candidate_ids = [[embedding_lookup.get(tuple(embedding), \"Not Found\") for embedding in candidate_list] \n",
    "                 for candidate_list in df[\"candidate_docs\"]]\n",
    "\n",
    "# Retrieve item IDs for slate_docs_feature\n",
    "slate_item_ids = [[embedding_lookup.get(tuple(embedding), \"Not Found\") for embedding in slate_list] \n",
    "                  for slate_list in df[\"slate_docs_feature\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_ids_and_titles(item_ids, news_df):\n",
    "    # Create a dictionary of itemId -> title for faster lookup\n",
    "    item_to_title = dict(zip(news_df[\"itemId\"], news_df[\"title\"]))\n",
    "    \n",
    "    # Retrieve the titles for each item_id\n",
    "    item_titles = [(item_id, item_to_title.get(item_id, \"Title not found\")) for item_id in item_ids]\n",
    "    \n",
    "    return item_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group-level averages:\n",
      "                             initial_user_state_tuple  group_mean_hit\n",
      "0   (-0.0259791798889637, 0.5851805210113525, 0.00...        0.250000\n",
      "1   (0.049241334199905396, 0.23491184413433075, 0....        0.333333\n",
      "2   (0.06917192041873932, 0.22154483199119568, 0.0...        0.000000\n",
      "3   (0.0808219462633133, 0.280564546585083, -0.035...        0.142857\n",
      "4   (0.11326862126588821, 0.38108861446380615, -0....        0.142857\n",
      "..                                                ...             ...\n",
      "95  (0.3616601228713989, 0.1373804360628128, 0.021...        0.000000\n",
      "96  (0.3617928624153137, 0.11457429826259613, 0.01...        0.000000\n",
      "97  (0.36578845977783203, 0.042487733066082, 0.081...        0.500000\n",
      "98  (0.39353927969932556, 0.0698390007019043, -0.0...        0.000000\n",
      "99  (0.3967207372188568, 0.18588663637638092, -0.0...        0.000000\n",
      "\n",
      "[100 rows x 2 columns]\n",
      "\n",
      "Overall average:\n",
      "0.1825281385281385\n"
     ]
    }
   ],
   "source": [
    "df= df[df['llm_slate'].apply(lambda x: len(x) > 0)].copy()\n",
    "df['initial_user_state_tuple'] = df['initial_user_state'].apply(tuple)\n",
    "\n",
    "# Step 2: Group by initial_user_state and calculate the mean of 'hit' for each group\n",
    "grouped_means_rl_llm = df.groupby('initial_user_state_tuple')['hit'].mean().reset_index()\n",
    "grouped_means_rl_llm.rename(columns={'hit': 'group_mean_hit'}, inplace=True)\n",
    "\n",
    "# Step 3: Calculate the overall average of the group means\n",
    "overall_mean = grouped_means_rl_llm['group_mean_hit'].mean()\n",
    "\n",
    "# Display the results\n",
    "print(\"Group-level averages:\")\n",
    "print(grouped_means_rl_llm)\n",
    "print(\"\\nOverall average:\")\n",
    "print(overall_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rl_slates'] = [\n",
    "    [embedding_lookup.get(tuple(embedding), \"Not Found\") for embedding in slate_list]\n",
    "    for slate_list in df['slate_docs_feature']\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rl_hit'] = df.apply(lambda row: 1 if row['original_click'] in row['rl_slates'] else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group-level averages:\n",
      "                             initial_user_state_tuple  group_mean_rl_hit\n",
      "0   (-0.0259791798889637, 0.5851805210113525, 0.00...           0.000000\n",
      "1   (0.049241334199905396, 0.23491184413433075, 0....           0.000000\n",
      "2   (0.06917192041873932, 0.22154483199119568, 0.0...           0.000000\n",
      "3   (0.0808219462633133, 0.280564546585083, -0.035...           0.000000\n",
      "4   (0.11326862126588821, 0.38108861446380615, -0....           0.142857\n",
      "..                                                ...                ...\n",
      "95  (0.3616601228713989, 0.1373804360628128, 0.021...           0.000000\n",
      "96  (0.3617928624153137, 0.11457429826259613, 0.01...           0.000000\n",
      "97  (0.36578845977783203, 0.042487733066082, 0.081...           0.000000\n",
      "98  (0.39353927969932556, 0.0698390007019043, -0.0...           0.000000\n",
      "99  (0.3967207372188568, 0.18588663637638092, -0.0...           0.000000\n",
      "\n",
      "[100 rows x 2 columns]\n",
      "\n",
      "Overall average:\n",
      "0.047432900432900424\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 2: Group by initial_user_state and calculate the mean of 'hit' for each group\n",
    "grouped_means_rl = df.groupby('initial_user_state_tuple')['rl_hit'].mean().reset_index()\n",
    "grouped_means_rl.rename(columns={'rl_hit': 'group_mean_rl_hit'}, inplace=True)\n",
    "\n",
    "# Step 3: Calculate the overall average of the group means\n",
    "overall_mean = grouped_means_rl['group_mean_rl_hit'].mean()\n",
    "\n",
    "# Display the results\n",
    "print(\"Group-level averages:\")\n",
    "print(grouped_means_rl)\n",
    "print(\"\\nOverall average:\")\n",
    "print(overall_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# df_filtered = df_filtered.reset_index(drop=True)\n",
    "\n",
    "def hybrid_slate_optimization(row):\n",
    "    \"\"\"\n",
    "    Replaces the 3 least relevant items in the slate using a hybrid BM25 + cosine similarity approach.\n",
    "\n",
    "    Args:\n",
    "        row: A row from the DataFrame (passed via df.apply).\n",
    "\n",
    "    Returns:\n",
    "        Updated slate as a list of item IDs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve item IDs using the embedding lookup\n",
    "    slate_item_ids = [embedding_lookup.get(tuple(embedding), \"Not Found\") for embedding in row[\"slate_docs_feature\"]]\n",
    "    candidate_item_ids = [embedding_lookup.get(tuple(embedding), \"Not Found\") for embedding in row[\"candidate_docs\"]]\n",
    "\n",
    "    # Get titles for slate and candidate items\n",
    "    slate_titles = [title for _, title in get_item_ids_and_titles(slate_item_ids, news_df)]\n",
    "    candidate_titles = [title for _, title in get_item_ids_and_titles(candidate_item_ids, news_df)]\n",
    "    \n",
    "\n",
    "    # Tokenize titles for BM25\n",
    "    slate_tokens = [text.split() for text in slate_titles]\n",
    "    candidate_tokens = [text.split() for text in candidate_titles]\n",
    "    \n",
    "    bm25 = BM25Okapi(candidate_tokens)\n",
    "    bm25_scores = np.array([bm25.get_scores(tokens) for tokens in slate_tokens])  # (N, M)\n",
    "\n",
    "    # Compute Cosine Similarity scores\n",
    "    candidate_docs_matrix = np.array(row[\"candidate_docs\"])  # Convert to 2D numpy array\n",
    "    candidate_docs_matrix = np.vstack(row[\"candidate_docs\"])\n",
    "    slate_docs_feature_matrix = np.array(row[\"slate_docs_feature\"])  # Convert to 2D numpy array\n",
    "    slate_docs_feature_matrix = np.vstack(row[\"slate_docs_feature\"])\n",
    "\n",
    "    similarity_matrix = cosine_similarity(slate_docs_feature_matrix,candidate_docs_matrix)  # (M, N)\n",
    "    \n",
    "   \n",
    "   \n",
    "    # Normalize and Combine Scores\n",
    "    lambda_weight = 1.0  # Adjust balance between BM25 and cosine similarity\n",
    "    bm25_norm = (bm25_scores - bm25_scores.min()) / (bm25_scores.max() - bm25_scores.min() + 1e-6)\n",
    "    # bm25_norm = bm25_norm.T \n",
    "    cosine_norm = (similarity_matrix - similarity_matrix.min()) / (similarity_matrix.max() - similarity_matrix.min() + 1e-6)\n",
    "  \n",
    "    final_scores = lambda_weight * bm25_norm + (1 - lambda_weight) * cosine_norm  # (N, M)\n",
    "    \n",
    "\n",
    "    # Identify 3 least relevant slate items\n",
    "    avg_slate_relevance = final_scores.mean(axis=1)  # Average score per slate item\n",
    "    least_relevant_indices = np.argsort(avg_slate_relevance)[:9]  # Indices of 3 least relevant slate items\n",
    "\n",
    "    # Select 3 best candidates\n",
    "    best_candidate_indices = np.argsort(final_scores.max(axis=0))[-9:]  # Indices of top 3 candidates\n",
    "\n",
    "    # Replace the least relevant slate items with best candidates\n",
    "    updated_slate_item_ids = slate_item_ids[:]\n",
    "    for slate_idx, candidate_idx in zip(least_relevant_indices, best_candidate_indices):\n",
    "        updated_slate_item_ids[slate_idx] = candidate_item_ids[candidate_idx]  # Replace with best candidate ID\n",
    "    \n",
    "\n",
    "    return updated_slate_item_ids\n",
    "\n",
    "# Apply function to DataFrame\n",
    "\n",
    "df[\"slate_reranked\"] = df.apply(hybrid_slate_optimization, axis=1)\n",
    "\n",
    "\n",
    "# # Save the updated DataFrame\n",
    "# df.to_csv(\"updated_slate_data.csv\", index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reranked_hit'] = df.apply(lambda row: 1 if row['original_click'] in row['slate_reranked'] else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group-level averages:\n",
      "                             initial_user_state_tuple  group_mean_reranked_hit\n",
      "0   (-0.0259791798889637, 0.5851805210113525, 0.00...                 0.000000\n",
      "1   (0.049241334199905396, 0.23491184413433075, 0....                 0.000000\n",
      "2   (0.06917192041873932, 0.22154483199119568, 0.0...                 0.000000\n",
      "3   (0.0808219462633133, 0.280564546585083, -0.035...                 0.000000\n",
      "4   (0.11326862126588821, 0.38108861446380615, -0....                 0.142857\n",
      "..                                                ...                      ...\n",
      "95  (0.3616601228713989, 0.1373804360628128, 0.021...                 0.000000\n",
      "96  (0.3617928624153137, 0.11457429826259613, 0.01...                 0.000000\n",
      "97  (0.36578845977783203, 0.042487733066082, 0.081...                 0.000000\n",
      "98  (0.39353927969932556, 0.0698390007019043, -0.0...                 0.000000\n",
      "99  (0.3967207372188568, 0.18588663637638092, -0.0...                 0.000000\n",
      "\n",
      "[100 rows x 2 columns]\n",
      "\n",
      "Overall average:\n",
      "0.0440995670995671\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 2: Group by initial_user_state and calculate the mean of 'hit' for each group\n",
    "grouped_means_bm25 = df.groupby('initial_user_state_tuple')['reranked_hit'].mean().reset_index()\n",
    "grouped_means_bm25.rename(columns={'reranked_hit': 'group_mean_reranked_hit'}, inplace=True)\n",
    "\n",
    "# Step 3: Calculate the overall average of the group means\n",
    "overall_mean = grouped_means_bm25['group_mean_reranked_hit'].mean()\n",
    "\n",
    "# Display the results\n",
    "print(\"Group-level averages:\")\n",
    "print(grouped_means_bm25)\n",
    "print(\"\\nOverall average:\")\n",
    "print(overall_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average S-Recall for llm_slate:\n",
      "Category Level: 0.3851, Subcategory Level: 0.6452\n",
      "\n",
      "Average S-Recall for rl_slates:\n",
      "Category Level: 0.5696, Subcategory Level: 0.8658\n",
      "\n",
      "Average S-Recall for slate_reranked:\n",
      "Category Level: 0.5266, Subcategory Level: 0.7889\n",
      "\n",
      "Number of subcategories for each category:\n",
      "         category  subcategory_count\n",
      "0           autos                 25\n",
      "1   entertainment                 14\n",
      "2         finance                 33\n",
      "3    foodanddrink                 16\n",
      "4           games                  1\n",
      "5          health                 23\n",
      "6            kids                  6\n",
      "7       lifestyle                 53\n",
      "8      middleeast                  1\n",
      "9          movies                  7\n",
      "10          music                 11\n",
      "11           news                 38\n",
      "12   northamerica                  1\n",
      "13         sports                 34\n",
      "14         travel                 16\n",
      "15             tv                 10\n",
      "16          video                 15\n",
      "17        weather                  3\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary for quick lookup of category and subcategory\n",
    "item_to_category = dict(zip(news_df['itemId'], news_df['category']))\n",
    "item_to_subcategory = dict(zip(news_df['itemId'], news_df['subcategory']))\n",
    "\n",
    "# Calculate total unique categories and subcategories in the dataset\n",
    "total_categories = news_df['category'].nunique()\n",
    "total_subcategories = news_df['subcategory'].nunique()\n",
    "\n",
    "# Function to calculate diversity metrics for a given slate\n",
    "def calculate_diversity(slate, item_to_category, item_to_subcategory):\n",
    "    categories = set()\n",
    "    subcategories = set()\n",
    "    \n",
    "    for item in slate:\n",
    "        if item in item_to_category:\n",
    "            categories.add(item_to_category[item])\n",
    "        if item in item_to_subcategory:\n",
    "            subcategories.add(item_to_subcategory[item])\n",
    "    \n",
    "    return len(categories), len(subcategories)\n",
    "\n",
    "# Function to calculate S-Recall as a ratio\n",
    "def calculate_s_recall(df, column, item_to_category, item_to_subcategory, total_categories, total_subcategories):\n",
    "    results = []\n",
    "    \n",
    "    for slate in df[column]:\n",
    "        category_diversity, subcategory_diversity = calculate_diversity(slate, item_to_category, item_to_subcategory)\n",
    "        \n",
    "        # Calculate S-Recall as a ratio\n",
    "        s_recall_category = category_diversity / len(slate)\n",
    "        s_recall_subcategory = subcategory_diversity / len(slate)\n",
    "        \n",
    "        results.append((s_recall_category, s_recall_subcategory))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Calculate S-Recall for each slate column\n",
    "llm_s_recall = calculate_s_recall(df, 'llm_slate', item_to_category, item_to_subcategory, total_categories, total_subcategories)\n",
    "rl_s_recall = calculate_s_recall(df, 'rl_slates', item_to_category, item_to_subcategory, total_categories, total_subcategories)\n",
    "slate_reranked_recall = calculate_s_recall(df, 'slate_reranked', item_to_category, item_to_subcategory, total_categories, total_subcategories)\n",
    "\n",
    "# Calculate average S-Recall for each column\n",
    "def calculate_average_s_recall(s_recall_results):\n",
    "    avg_category = sum([x[0] for x in s_recall_results]) / len(s_recall_results)\n",
    "    avg_subcategory = sum([x[1] for x in s_recall_results]) / len(s_recall_results)\n",
    "    return avg_category, avg_subcategory\n",
    "\n",
    "llm_avg_category, llm_avg_subcategory = calculate_average_s_recall(llm_s_recall)\n",
    "rl_avg_category, rl_avg_subcategory = calculate_average_s_recall(rl_s_recall)\n",
    "slate_avg_category, slate_avg_subcategory = calculate_average_s_recall(slate_reranked_recall)\n",
    "\n",
    "# Print average S-Recall for each column\n",
    "print(\"Average S-Recall for llm_slate:\")\n",
    "print(f\"Category Level: {llm_avg_category:.4f}, Subcategory Level: {llm_avg_subcategory:.4f}\")\n",
    "\n",
    "print(\"\\nAverage S-Recall for rl_slates:\")\n",
    "print(f\"Category Level: {rl_avg_category:.4f}, Subcategory Level: {rl_avg_subcategory:.4f}\")\n",
    "\n",
    "print(\"\\nAverage S-Recall for slate_reranked:\")\n",
    "print(f\"Category Level: {slate_avg_category:.4f}, Subcategory Level: {slate_avg_subcategory:.4f}\")\n",
    "\n",
    "# Calculate number of subcategories for each category\n",
    "subcategory_count = news_df.groupby('category')['subcategory'].nunique().reset_index()\n",
    "subcategory_count.columns = ['category', 'subcategory_count']\n",
    "\n",
    "print(\"\\nNumber of subcategories for each category:\")\n",
    "print(subcategory_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average S-Recall for llm_slate:\n",
      "Category Level: 0.3851, Subcategory Level: 0.0700\n",
      "\n",
      "Average S-Recall for rl_slates:\n",
      "Category Level: 0.5696, Subcategory Level: 0.0598\n",
      "\n",
      "Average S-Recall for slate_reranked:\n",
      "Category Level: 0.5266, Subcategory Level: 0.0577\n",
      "\n",
      "Number of subcategories for each category:\n",
      "         category  subcategory_count\n",
      "0           autos                 25\n",
      "1   entertainment                 14\n",
      "2         finance                 33\n",
      "3    foodanddrink                 16\n",
      "4           games                  1\n",
      "5          health                 23\n",
      "6            kids                  6\n",
      "7       lifestyle                 53\n",
      "8      middleeast                  1\n",
      "9          movies                  7\n",
      "10          music                 11\n",
      "11           news                 38\n",
      "12   northamerica                  1\n",
      "13         sports                 34\n",
      "14         travel                 16\n",
      "15             tv                 10\n",
      "16          video                 15\n",
      "17        weather                  3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a dictionary to map categories to their subcategories\n",
    "category_to_subcategories = news_df.groupby('category')['subcategory'].unique().to_dict()\n",
    "\n",
    "# Function to calculate diversity metrics for a given slate\n",
    "def calculate_diversity(slate, item_to_category, item_to_subcategory):\n",
    "    categories = set()\n",
    "    subcategories = set()\n",
    "    \n",
    "    for item in slate:\n",
    "        if item in item_to_category:\n",
    "            categories.add(item_to_category[item])\n",
    "        if item in item_to_subcategory:\n",
    "            subcategories.add(item_to_subcategory[item])\n",
    "    \n",
    "    return categories, subcategories\n",
    "\n",
    "# Function to calculate S-Recall as a ratio\n",
    "def calculate_s_recall(df, column, item_to_category, item_to_subcategory, category_to_subcategories):\n",
    "    results = []\n",
    "    \n",
    "    for slate in df[column]:\n",
    "        categories_in_slate, subcategories_in_slate = calculate_diversity(slate, item_to_category, item_to_subcategory)\n",
    "        \n",
    "        # Calculate S-Recall at category level\n",
    "        s_recall_category = len(categories_in_slate) / len(slate)\n",
    "        \n",
    "        # Calculate S-Recall at subcategory level (contextual to categories in the slate)\n",
    "        total_subcategories_in_categories = set()\n",
    "        for category in categories_in_slate:\n",
    "            total_subcategories_in_categories.update(category_to_subcategories[category])\n",
    "        \n",
    "        s_recall_subcategory = len(subcategories_in_slate) / len(total_subcategories_in_categories)\n",
    "        \n",
    "        results.append((s_recall_category, s_recall_subcategory))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Calculate S-Recall for each slate column\n",
    "llm_s_recall = calculate_s_recall(df, 'llm_slate', item_to_category, item_to_subcategory, category_to_subcategories)\n",
    "rl_s_recall = calculate_s_recall(df, 'rl_slates', item_to_category, item_to_subcategory, category_to_subcategories)\n",
    "slate_reranked_recall = calculate_s_recall(df, 'slate_reranked', item_to_category, item_to_subcategory, category_to_subcategories)\n",
    "\n",
    "# Calculate average S-Recall for each column\n",
    "def calculate_average_s_recall(s_recall_results):\n",
    "    avg_category = sum([x[0] for x in s_recall_results]) / len(s_recall_results)\n",
    "    avg_subcategory = sum([x[1] for x in s_recall_results]) / len(s_recall_results)\n",
    "    return avg_category, avg_subcategory\n",
    "\n",
    "llm_avg_category, llm_avg_subcategory = calculate_average_s_recall(llm_s_recall)\n",
    "rl_avg_category, rl_avg_subcategory = calculate_average_s_recall(rl_s_recall)\n",
    "slate_avg_category, slate_avg_subcategory = calculate_average_s_recall(slate_reranked_recall)\n",
    "\n",
    "# Print average S-Recall for each column\n",
    "print(\"Average S-Recall for llm_slate:\")\n",
    "print(f\"Category Level: {llm_avg_category:.4f}, Subcategory Level: {llm_avg_subcategory:.4f}\")\n",
    "\n",
    "print(\"\\nAverage S-Recall for rl_slates:\")\n",
    "print(f\"Category Level: {rl_avg_category:.4f}, Subcategory Level: {rl_avg_subcategory:.4f}\")\n",
    "\n",
    "print(\"\\nAverage S-Recall for slate_reranked:\")\n",
    "print(f\"Category Level: {slate_avg_category:.4f}, Subcategory Level: {slate_avg_subcategory:.4f}\")\n",
    "\n",
    "# Calculate number of subcategories for each category\n",
    "subcategory_count = news_df.groupby('category')['subcategory'].nunique().reset_index()\n",
    "subcategory_count.columns = ['category', 'subcategory_count']\n",
    "\n",
    "print(\"\\nNumber of subcategories for each category:\")\n",
    "print(subcategory_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_data[\"observed_state\"] = category_data[\"observed_state\"].apply(lambda x: tuple(x) if x is not None else ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicked_data_user_history = category_data.merge(\n",
    "    df,\n",
    "    left_on=['click', 'observed_state'],\n",
    "    right_on=['original_click', 'initial_user_state_tuple'],\n",
    "    how='right'  # Use 'inner' to keep only matching rows\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract titles from the list of tuples\n",
    "def extract_titles(item_tuples):\n",
    "    return [title for (_, title) in item_tuples]\n",
    "\n",
    "# Function to compute BLEU score between two lists of titles\n",
    "def compute_bleu_score(reference, candidate):\n",
    "    reference_tokens = [word_tokenize(str(title)) for title in reference]\n",
    "    candidate_tokens = word_tokenize(str(candidate[0]))  # Ensure candidate is a single tokenized sentence\n",
    "    \n",
    "    # Compute BLEU score\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    return sentence_bleu(reference_tokens, candidate_tokens, smoothing_function=smoothing)\n",
    "# Compute BLEU scores for each row\n",
    "clicked_data_user_history['bleu_rl_vs_presented'] = clicked_data_user_history.apply(\n",
    "    lambda row: compute_bleu_score(\n",
    "        extract_titles(get_item_ids_and_titles(row['presented_slate'], news_df)),\n",
    "        extract_titles(get_item_ids_and_titles(row['rl_slates'], news_df)),  # Replace None with your news_df# Replace None with your news_df\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "clicked_data_user_history['bleu_llm_vs_presented'] = clicked_data_user_history.apply(\n",
    "    lambda row: compute_bleu_score(\n",
    "        extract_titles(get_item_ids_and_titles(row['presented_slate'], news_df)) ,\n",
    "        extract_titles(get_item_ids_and_titles(row['llm_slate'], news_df)),  # Replace None with your news_df# Replace None with your news_df\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "clicked_data_user_history['bleu_reranked_vs_presented'] = clicked_data_user_history.apply(\n",
    "    lambda row: compute_bleu_score(\n",
    "        extract_titles(get_item_ids_and_titles(row['presented_slate'], news_df)) ,\n",
    "        extract_titles(get_item_ids_and_titles(row['slate_reranked'], news_df)),  # Replace None with your news_df# Replace None with your news_df\n",
    "    ), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group-level averages:\n",
      "                             initial_user_state_tuple  \\\n",
      "0   (-0.0259791798889637, 0.5851805210113525, 0.00...   \n",
      "1   (0.049241334199905396, 0.23491184413433075, 0....   \n",
      "2   (0.06917192041873932, 0.22154483199119568, 0.0...   \n",
      "3   (0.0808219462633133, 0.280564546585083, -0.035...   \n",
      "4   (0.11326862126588821, 0.38108861446380615, -0....   \n",
      "..                                                ...   \n",
      "95  (0.3616601228713989, 0.1373804360628128, 0.021...   \n",
      "96  (0.3617928624153137, 0.11457429826259613, 0.01...   \n",
      "97  (0.36578845977783203, 0.042487733066082, 0.081...   \n",
      "98  (0.39353927969932556, 0.0698390007019043, -0.0...   \n",
      "99  (0.3967207372188568, 0.18588663637638092, -0.0...   \n",
      "\n",
      "    group_mean_bleu_rl_vs_presented  \n",
      "0                          1.000000  \n",
      "1                          0.348927  \n",
      "2                          0.020448  \n",
      "3                          0.027184  \n",
      "4                          0.109868  \n",
      "..                              ...  \n",
      "95                         0.024141  \n",
      "96                         0.021650  \n",
      "97                         0.347751  \n",
      "98                         0.346025  \n",
      "99                         0.031673  \n",
      "\n",
      "[100 rows x 2 columns]\n",
      "\n",
      "Overall average:\n",
      "0.2344573408738\n"
     ]
    }
   ],
   "source": [
    "grouped_means_bleu_rl = clicked_data_user_history.groupby('initial_user_state_tuple')['bleu_rl_vs_presented'].mean().reset_index()\n",
    "grouped_means_bleu_rl.rename(columns={'bleu_rl_vs_presented': 'group_mean_bleu_rl_vs_presented'}, inplace=True)\n",
    "\n",
    "# Step 3: Calculate the overall average of the group means\n",
    "overall_mean = grouped_means_bleu_rl['group_mean_bleu_rl_vs_presented'].mean()\n",
    "\n",
    "# Display the results\n",
    "print(\"Group-level averages:\")\n",
    "print(grouped_means_bleu_rl)\n",
    "print(\"\\nOverall average:\")\n",
    "print(overall_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group-level averages:\n",
      "                             initial_user_state_tuple  \\\n",
      "0   (-0.0259791798889637, 0.5851805210113525, 0.00...   \n",
      "1   (0.049241334199905396, 0.23491184413433075, 0....   \n",
      "2   (0.06917192041873932, 0.22154483199119568, 0.0...   \n",
      "3   (0.0808219462633133, 0.280564546585083, -0.035...   \n",
      "4   (0.11326862126588821, 0.38108861446380615, -0....   \n",
      "..                                                ...   \n",
      "95  (0.3616601228713989, 0.1373804360628128, 0.021...   \n",
      "96  (0.3617928624153137, 0.11457429826259613, 0.01...   \n",
      "97  (0.36578845977783203, 0.042487733066082, 0.081...   \n",
      "98  (0.39353927969932556, 0.0698390007019043, -0.0...   \n",
      "99  (0.3967207372188568, 0.18588663637638092, -0.0...   \n",
      "\n",
      "    group_mean_bleu_reranked_vs_presented  \n",
      "0                                0.274076  \n",
      "1                                0.348927  \n",
      "2                                0.019641  \n",
      "3                                0.168887  \n",
      "4                                0.308249  \n",
      "..                                    ...  \n",
      "95                               0.037544  \n",
      "96                               0.216883  \n",
      "97                               0.683414  \n",
      "98                               0.348115  \n",
      "99                               0.026354  \n",
      "\n",
      "[100 rows x 2 columns]\n",
      "\n",
      "Overall average:\n",
      "0.22636164752225274\n"
     ]
    }
   ],
   "source": [
    "grouped_means_bleu_bm25 = clicked_data_user_history.groupby('initial_user_state_tuple')['bleu_reranked_vs_presented'].mean().reset_index()\n",
    "grouped_means_bleu_bm25.rename(columns={'bleu_reranked_vs_presented': 'group_mean_bleu_reranked_vs_presented'}, inplace=True)\n",
    "\n",
    "# Step 3: Calculate the overall average of the group means\n",
    "overall_mean = grouped_means_bleu_bm25['group_mean_bleu_reranked_vs_presented'].mean()\n",
    "\n",
    "# Display the results\n",
    "print(\"Group-level averages:\")\n",
    "print(grouped_means_bleu_bm25)\n",
    "print(\"\\nOverall average:\")\n",
    "print(overall_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group-level averages:\n",
      "                             initial_user_state_tuple  \\\n",
      "0   (-0.0259791798889637, 0.5851805210113525, 0.00...   \n",
      "1   (0.049241334199905396, 0.23491184413433075, 0....   \n",
      "2   (0.06917192041873932, 0.22154483199119568, 0.0...   \n",
      "3   (0.0808219462633133, 0.280564546585083, -0.035...   \n",
      "4   (0.11326862126588821, 0.38108861446380615, -0....   \n",
      "..                                                ...   \n",
      "95  (0.3616601228713989, 0.1373804360628128, 0.021...   \n",
      "96  (0.3617928624153137, 0.11457429826259613, 0.01...   \n",
      "97  (0.36578845977783203, 0.042487733066082, 0.081...   \n",
      "98  (0.39353927969932556, 0.0698390007019043, -0.0...   \n",
      "99  (0.3967207372188568, 0.18588663637638092, -0.0...   \n",
      "\n",
      "    group_mean_bleu_llm_vs_presented  \n",
      "0                           1.000000  \n",
      "1                           0.021973  \n",
      "2                           0.024448  \n",
      "3                           0.041316  \n",
      "4                           0.603001  \n",
      "..                               ...  \n",
      "95                          0.018477  \n",
      "96                          0.023419  \n",
      "97                          0.364739  \n",
      "98                          0.022325  \n",
      "99                          1.000000  \n",
      "\n",
      "[100 rows x 2 columns]\n",
      "\n",
      "Overall average:\n",
      "0.4068742021401646\n"
     ]
    }
   ],
   "source": [
    "grouped_means_bleu_llm = clicked_data_user_history.groupby('initial_user_state_tuple')['bleu_llm_vs_presented'].mean().reset_index()\n",
    "grouped_means_bleu_llm.rename(columns={'bleu_llm_vs_presented': 'group_mean_bleu_llm_vs_presented'}, inplace=True)\n",
    "\n",
    "# Step 3: Calculate the overall average of the group means\n",
    "overall_mean = grouped_means_bleu_llm['group_mean_bleu_llm_vs_presented'].mean()\n",
    "\n",
    "# Display the results\n",
    "print(\"Group-level averages:\")\n",
    "print(grouped_means_bleu_llm)\n",
    "print(\"\\nOverall average:\")\n",
    "print(overall_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "feather_file_path_slateq= gen_slates_dir / \"slateq_llm_slates.feather\"\n",
    "df_slateq = pd.read_feather(feather_file_path_slateq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group-level averages:\n",
      "                             initial_user_state_tuple  group_mean_hit\n",
      "0   (0.13400940597057343, 0.2015896886587143, -0.0...            0.00\n",
      "1   (0.15157544612884521, 0.33397215604782104, -0....            1.00\n",
      "2   (0.15727820992469788, 0.2477571815252304, -0.1...            0.00\n",
      "3   (0.1577419489622116, 0.21108318865299225, -0.0...            1.00\n",
      "4   (0.15791304409503937, 0.29125070571899414, -0....            0.00\n",
      "..                                                ...             ...\n",
      "95  (0.2903972268104553, 0.15165531635284424, -0.0...            0.25\n",
      "96  (0.2935115098953247, 0.13266007602214813, -0.1...            0.00\n",
      "97  (0.29664674401283264, 0.14605002105236053, 0.0...            0.00\n",
      "98  (0.29784202575683594, 0.2651831805706024, 0.01...            0.00\n",
      "99  (0.2985374331474304, 0.1253967434167862, 0.048...            0.00\n",
      "\n",
      "[100 rows x 2 columns]\n",
      "\n",
      "Overall average:\n",
      "0.1322950558213716\n"
     ]
    }
   ],
   "source": [
    "df_slateq= df_slateq[df_slateq['llm_slateq_slate'].apply(lambda x: len(x) > 0)].copy()\n",
    "df_slateq['initial_user_state_tuple'] = df_slateq['initial_user_state'].apply(tuple)\n",
    "\n",
    "# Step 2: Group by initial_user_state and calculate the mean of 'hit' for each group\n",
    "grouped_means_slateq_llm = df_slateq.groupby('initial_user_state_tuple')['slateq_hit'].mean().reset_index()\n",
    "grouped_means_slateq_llm.rename(columns={'slateq_hit': 'group_mean_hit'}, inplace=True)\n",
    "\n",
    "# Step 3: Calculate the overall average of the group means\n",
    "overall_mean = grouped_means_slateq_llm['group_mean_hit'].mean()\n",
    "\n",
    "# Display the results\n",
    "print(\"Group-level averages:\")\n",
    "print(grouped_means_slateq_llm)\n",
    "print(\"\\nOverall average:\")\n",
    "print(overall_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slateq['slateq_slates'] = [\n",
    "    [embedding_lookup.get(tuple(embedding), \"Not Found\") for embedding in slate_list]\n",
    "    for slate_list in df_slateq['slate_docs_feature']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slateq['actual_slateq_hit'] = df_slateq.apply(lambda row: 1 if row['original_click'] in row['slateq_slates'] else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group-level averages:\n",
      "                             initial_user_state_tuple  \\\n",
      "0   (0.13400940597057343, 0.2015896886587143, -0.0...   \n",
      "1   (0.15157544612884521, 0.33397215604782104, -0....   \n",
      "2   (0.15727820992469788, 0.2477571815252304, -0.1...   \n",
      "3   (0.1577419489622116, 0.21108318865299225, -0.0...   \n",
      "4   (0.15791304409503937, 0.29125070571899414, -0....   \n",
      "..                                                ...   \n",
      "95  (0.2903972268104553, 0.15165531635284424, -0.0...   \n",
      "96  (0.2935115098953247, 0.13266007602214813, -0.1...   \n",
      "97  (0.29664674401283264, 0.14605002105236053, 0.0...   \n",
      "98  (0.29784202575683594, 0.2651831805706024, 0.01...   \n",
      "99  (0.2985374331474304, 0.1253967434167862, 0.048...   \n",
      "\n",
      "    group_mean_actual_slateq_hit  \n",
      "0                            0.0  \n",
      "1                            0.0  \n",
      "2                            0.5  \n",
      "3                            0.0  \n",
      "4                            0.0  \n",
      "..                           ...  \n",
      "95                           0.0  \n",
      "96                           0.0  \n",
      "97                           0.0  \n",
      "98                           0.5  \n",
      "99                           0.0  \n",
      "\n",
      "[100 rows x 2 columns]\n",
      "\n",
      "Overall average:\n",
      "0.06604835807467387\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Group by initial_user_state and calculate the mean of 'hit' for each group\n",
    "grouped_means_slateq = df_slateq.groupby('initial_user_state_tuple')['actual_slateq_hit'].mean().reset_index()\n",
    "grouped_means_slateq.rename(columns={'actual_slateq_hit': 'group_mean_actual_slateq_hit'}, inplace=True)\n",
    "\n",
    "# Step 3: Calculate the overall average of the group means\n",
    "overall_mean = grouped_means_slateq['group_mean_actual_slateq_hit'].mean()\n",
    "\n",
    "# Display the results\n",
    "print(\"Group-level averages:\")\n",
    "print(grouped_means_slateq)\n",
    "print(\"\\nOverall average:\")\n",
    "print(overall_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# df_filtered = df_filtered.reset_index(drop=True)\n",
    "\n",
    "def hybrid_slate_optimization(row):\n",
    "    \"\"\"\n",
    "    Replaces the 3 least relevant items in the slate using a hybrid BM25 + cosine similarity approach.\n",
    "\n",
    "    Args:\n",
    "        row: A row from the DataFrame (passed via df.apply).\n",
    "\n",
    "    Returns:\n",
    "        Updated slate as a list of item IDs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve item IDs using the embedding lookup\n",
    "    slate_item_ids = [embedding_lookup.get(tuple(embedding), \"Not Found\") for embedding in row[\"slate_docs_feature\"]]\n",
    "    candidate_item_ids = [embedding_lookup.get(tuple(embedding), \"Not Found\") for embedding in row[\"candidate_docs\"]]\n",
    "\n",
    "    # Get titles for slate and candidate items\n",
    "    slate_titles = [title for _, title in get_item_ids_and_titles(slate_item_ids, news_df)]\n",
    "    candidate_titles = [title for _, title in get_item_ids_and_titles(candidate_item_ids, news_df)]\n",
    "    \n",
    "\n",
    "    # Tokenize titles for BM25\n",
    "    slate_tokens = [text.split() for text in slate_titles]\n",
    "    candidate_tokens = [text.split() for text in candidate_titles]\n",
    "    \n",
    "    bm25 = BM25Okapi(candidate_tokens)\n",
    "    bm25_scores = np.array([bm25.get_scores(tokens) for tokens in slate_tokens])  # (N, M)\n",
    "\n",
    "    # Compute Cosine Similarity scores\n",
    "    candidate_docs_matrix = np.array(row[\"candidate_docs\"])  # Convert to 2D numpy array\n",
    "    candidate_docs_matrix = np.vstack(row[\"candidate_docs\"])\n",
    "    slate_docs_feature_matrix = np.array(row[\"slate_docs_feature\"])  # Convert to 2D numpy array\n",
    "    slate_docs_feature_matrix = np.vstack(row[\"slate_docs_feature\"])\n",
    "\n",
    "    similarity_matrix = cosine_similarity(slate_docs_feature_matrix,candidate_docs_matrix)  # (M, N)\n",
    "    \n",
    "   \n",
    "   \n",
    "    # Normalize and Combine Scores\n",
    "    lambda_weight = 1.0  # Adjust balance between BM25 and cosine similarity\n",
    "    bm25_norm = (bm25_scores - bm25_scores.min()) / (bm25_scores.max() - bm25_scores.min() + 1e-6)\n",
    "    # bm25_norm = bm25_norm.T \n",
    "    cosine_norm = (similarity_matrix - similarity_matrix.min()) / (similarity_matrix.max() - similarity_matrix.min() + 1e-6)\n",
    "  \n",
    "    final_scores = lambda_weight * bm25_norm + (1 - lambda_weight) * cosine_norm  # (N, M)\n",
    "    \n",
    "\n",
    "    # Identify 3 least relevant slate items\n",
    "    avg_slate_relevance = final_scores.mean(axis=1)  # Average score per slate item\n",
    "    least_relevant_indices = np.argsort(avg_slate_relevance)[:9]  # Indices of 3 least relevant slate items\n",
    "\n",
    "    # Select 3 best candidates\n",
    "    best_candidate_indices = np.argsort(final_scores.max(axis=0))[-9:]  # Indices of top 3 candidates\n",
    "\n",
    "    # Replace the least relevant slate items with best candidates\n",
    "    updated_slate_item_ids = slate_item_ids[:]\n",
    "    for slate_idx, candidate_idx in zip(least_relevant_indices, best_candidate_indices):\n",
    "        updated_slate_item_ids[slate_idx] = candidate_item_ids[candidate_idx]  # Replace with best candidate ID\n",
    "    \n",
    "\n",
    "    return updated_slate_item_ids\n",
    "\n",
    "# Apply function to DataFrame\n",
    "\n",
    "df_slateq[\"slateq_reranked\"] = df_slateq.apply(hybrid_slate_optimization, axis=1)\n",
    "\n",
    "\n",
    "# # Save the updated DataFrame\n",
    "# df.to_csv(\"updated_slate_data.csv\", index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slateq['slateq_reranked_hit'] = df_slateq.apply(lambda row: 1 if row['original_click'] in row['slateq_reranked'] else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group-level averages:\n",
      "                             initial_user_state_tuple  \\\n",
      "0   (0.13400940597057343, 0.2015896886587143, -0.0...   \n",
      "1   (0.15157544612884521, 0.33397215604782104, -0....   \n",
      "2   (0.15727820992469788, 0.2477571815252304, -0.1...   \n",
      "3   (0.1577419489622116, 0.21108318865299225, -0.0...   \n",
      "4   (0.15791304409503937, 0.29125070571899414, -0....   \n",
      "..                                                ...   \n",
      "95  (0.2903972268104553, 0.15165531635284424, -0.0...   \n",
      "96  (0.2935115098953247, 0.13266007602214813, -0.1...   \n",
      "97  (0.29664674401283264, 0.14605002105236053, 0.0...   \n",
      "98  (0.29784202575683594, 0.2651831805706024, 0.01...   \n",
      "99  (0.2985374331474304, 0.1253967434167862, 0.048...   \n",
      "\n",
      "    group_mean_slateq_reranked_hit  \n",
      "0                              0.0  \n",
      "1                              0.0  \n",
      "2                              0.5  \n",
      "3                              0.0  \n",
      "4                              0.0  \n",
      "..                             ...  \n",
      "95                             0.0  \n",
      "96                             0.0  \n",
      "97                             0.0  \n",
      "98                             0.5  \n",
      "99                             0.0  \n",
      "\n",
      "[100 rows x 2 columns]\n",
      "\n",
      "Overall average:\n",
      "0.053715024741340524\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Group by initial_user_state and calculate the mean of 'hit' for each group\n",
    "grouped_means_bm25_slateq = df_slateq.groupby('initial_user_state_tuple')['slateq_reranked_hit'].mean().reset_index()\n",
    "grouped_means_bm25_slateq.rename(columns={'slateq_reranked_hit': 'group_mean_slateq_reranked_hit'}, inplace=True)\n",
    "\n",
    "# Step 3: Calculate the overall average of the group means\n",
    "overall_mean = grouped_means_bm25_slateq['group_mean_slateq_reranked_hit'].mean()\n",
    "\n",
    "# Display the results\n",
    "print(\"Group-level averages:\")\n",
    "print(grouped_means_bm25_slateq)\n",
    "print(\"\\nOverall average:\")\n",
    "print(overall_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average S-Recall for llm_slate:\n",
      "Category Level: 0.5199, Subcategory Level: 0.7714\n",
      "\n",
      "Average S-Recall for rl_slates:\n",
      "Category Level: 0.6162, Subcategory Level: 0.8601\n",
      "\n",
      "Average S-Recall for slate_reranked:\n",
      "Category Level: 0.5789, Subcategory Level: 0.7809\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary for quick lookup of category and subcategory\n",
    "item_to_category = dict(zip(news_df['itemId'], news_df['category']))\n",
    "item_to_subcategory = dict(zip(news_df['itemId'], news_df['subcategory']))\n",
    "\n",
    "# Calculate total unique categories and subcategories in the dataset\n",
    "total_categories = news_df['category'].nunique()\n",
    "total_subcategories = news_df['subcategory'].nunique()\n",
    "\n",
    "# Function to calculate diversity metrics for a given slate\n",
    "def calculate_diversity(slate, item_to_category, item_to_subcategory):\n",
    "    categories = set()\n",
    "    subcategories = set()\n",
    "    \n",
    "    for item in slate:\n",
    "        if item in item_to_category:\n",
    "            categories.add(item_to_category[item])\n",
    "        if item in item_to_subcategory:\n",
    "            subcategories.add(item_to_subcategory[item])\n",
    "    \n",
    "    return len(categories), len(subcategories)\n",
    "\n",
    "# Function to calculate S-Recall as a ratio\n",
    "def calculate_s_recall(df, column, item_to_category, item_to_subcategory, total_categories, total_subcategories):\n",
    "    results = []\n",
    "    \n",
    "    for slate in df[column]:\n",
    "        category_diversity, subcategory_diversity = calculate_diversity(slate, item_to_category, item_to_subcategory)\n",
    "        \n",
    "        # Calculate S-Recall as a ratio\n",
    "        s_recall_category = category_diversity / len(slate)\n",
    "        s_recall_subcategory = subcategory_diversity / len(slate)\n",
    "        \n",
    "        results.append((s_recall_category, s_recall_subcategory))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Calculate S-Recall for each slate column\n",
    "llm_s_recall = calculate_s_recall(df_slateq, 'llm_slateq_slate', item_to_category, item_to_subcategory, total_categories, total_subcategories)\n",
    "rl_s_recall = calculate_s_recall(df_slateq, 'slateq_slates', item_to_category, item_to_subcategory, total_categories, total_subcategories)\n",
    "slate_reranked_recall = calculate_s_recall(df_slateq, 'slateq_reranked', item_to_category, item_to_subcategory, total_categories, total_subcategories)\n",
    "\n",
    "# Calculate average S-Recall for each column\n",
    "def calculate_average_s_recall(s_recall_results):\n",
    "    avg_category = sum([x[0] for x in s_recall_results]) / len(s_recall_results)\n",
    "    avg_subcategory = sum([x[1] for x in s_recall_results]) / len(s_recall_results)\n",
    "    return avg_category, avg_subcategory\n",
    "\n",
    "llm_avg_category, llm_avg_subcategory = calculate_average_s_recall(llm_s_recall)\n",
    "rl_avg_category, rl_avg_subcategory = calculate_average_s_recall(rl_s_recall)\n",
    "slate_avg_category, slate_avg_subcategory = calculate_average_s_recall(slate_reranked_recall)\n",
    "\n",
    "# Print average S-Recall for each column\n",
    "print(\"Average S-Recall for llm_slate:\")\n",
    "print(f\"Category Level: {llm_avg_category:.4f}, Subcategory Level: {llm_avg_subcategory:.4f}\")\n",
    "\n",
    "print(\"\\nAverage S-Recall for rl_slates:\")\n",
    "print(f\"Category Level: {rl_avg_category:.4f}, Subcategory Level: {rl_avg_subcategory:.4f}\")\n",
    "\n",
    "print(\"\\nAverage S-Recall for slate_reranked:\")\n",
    "print(f\"Category Level: {slate_avg_category:.4f}, Subcategory Level: {slate_avg_subcategory:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average S-Recall for llm_slate:\n",
      "Category Level: 0.5199, Subcategory Level: 0.0638\n",
      "\n",
      "Average S-Recall for rl_slates:\n",
      "Category Level: 0.6162, Subcategory Level: 0.0602\n",
      "\n",
      "Average S-Recall for slate_reranked:\n",
      "Category Level: 0.5789, Subcategory Level: 0.0575\n",
      "\n",
      "Number of subcategories for each category:\n",
      "         category  subcategory_count\n",
      "0           autos                 25\n",
      "1   entertainment                 14\n",
      "2         finance                 33\n",
      "3    foodanddrink                 16\n",
      "4           games                  1\n",
      "5          health                 23\n",
      "6            kids                  6\n",
      "7       lifestyle                 53\n",
      "8      middleeast                  1\n",
      "9          movies                  7\n",
      "10          music                 11\n",
      "11           news                 38\n",
      "12   northamerica                  1\n",
      "13         sports                 34\n",
      "14         travel                 16\n",
      "15             tv                 10\n",
      "16          video                 15\n",
      "17        weather                  3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a dictionary to map categories to their subcategories\n",
    "category_to_subcategories = news_df.groupby('category')['subcategory'].unique().to_dict()\n",
    "\n",
    "# Function to calculate diversity metrics for a given slate\n",
    "def calculate_diversity(slate, item_to_category, item_to_subcategory):\n",
    "    categories = set()\n",
    "    subcategories = set()\n",
    "    \n",
    "    for item in slate:\n",
    "        if item in item_to_category:\n",
    "            categories.add(item_to_category[item])\n",
    "        if item in item_to_subcategory:\n",
    "            subcategories.add(item_to_subcategory[item])\n",
    "    \n",
    "    return categories, subcategories\n",
    "\n",
    "# Function to calculate S-Recall as a ratio\n",
    "def calculate_s_recall(df, column, item_to_category, item_to_subcategory, category_to_subcategories):\n",
    "    results = []\n",
    "    \n",
    "    for slate in df[column]:\n",
    "        categories_in_slate, subcategories_in_slate = calculate_diversity(slate, item_to_category, item_to_subcategory)\n",
    "        \n",
    "        # Calculate S-Recall at category level\n",
    "        s_recall_category = len(categories_in_slate) / len(slate)\n",
    "        \n",
    "        # Calculate S-Recall at subcategory level (contextual to categories in the slate)\n",
    "        total_subcategories_in_categories = set()\n",
    "        for category in categories_in_slate:\n",
    "            total_subcategories_in_categories.update(category_to_subcategories[category])\n",
    "        \n",
    "        s_recall_subcategory = len(subcategories_in_slate) / len(total_subcategories_in_categories)\n",
    "        \n",
    "        results.append((s_recall_category, s_recall_subcategory))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Calculate S-Recall for each slate column\n",
    "llm_s_recall = calculate_s_recall(df_slateq, 'llm_slateq_slate', item_to_category, item_to_subcategory, category_to_subcategories)\n",
    "rl_s_recall = calculate_s_recall(df_slateq, 'slateq_slates', item_to_category, item_to_subcategory, category_to_subcategories)\n",
    "slate_reranked_recall = calculate_s_recall(df_slateq, 'slateq_reranked', item_to_category, item_to_subcategory, category_to_subcategories)\n",
    "\n",
    "# Calculate average S-Recall for each column\n",
    "def calculate_average_s_recall(s_recall_results):\n",
    "    avg_category = sum([x[0] for x in s_recall_results]) / len(s_recall_results)\n",
    "    avg_subcategory = sum([x[1] for x in s_recall_results]) / len(s_recall_results)\n",
    "    return avg_category, avg_subcategory\n",
    "\n",
    "llm_avg_category, llm_avg_subcategory = calculate_average_s_recall(llm_s_recall)\n",
    "rl_avg_category, rl_avg_subcategory = calculate_average_s_recall(rl_s_recall)\n",
    "slate_avg_category, slate_avg_subcategory = calculate_average_s_recall(slate_reranked_recall)\n",
    "\n",
    "# Print average S-Recall for each column\n",
    "print(\"Average S-Recall for llm_slate:\")\n",
    "print(f\"Category Level: {llm_avg_category:.4f}, Subcategory Level: {llm_avg_subcategory:.4f}\")\n",
    "\n",
    "print(\"\\nAverage S-Recall for rl_slates:\")\n",
    "print(f\"Category Level: {rl_avg_category:.4f}, Subcategory Level: {rl_avg_subcategory:.4f}\")\n",
    "\n",
    "print(\"\\nAverage S-Recall for slate_reranked:\")\n",
    "print(f\"Category Level: {slate_avg_category:.4f}, Subcategory Level: {slate_avg_subcategory:.4f}\")\n",
    "\n",
    "# Calculate number of subcategories for each category\n",
    "subcategory_count = news_df.groupby('category')['subcategory'].nunique().reset_index()\n",
    "subcategory_count.columns = ['category', 'subcategory_count']\n",
    "\n",
    "print(\"\\nNumber of subcategories for each category:\")\n",
    "print(subcategory_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicked_data_user_history = category_data.merge(\n",
    "    df_slateq,\n",
    "    left_on=['click', 'observed_state'],\n",
    "    right_on=['original_click', 'initial_user_state_tuple'],\n",
    "    how='right'  # Use 'inner' to keep only matching rows\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract titles from the list of tuples\n",
    "def extract_titles(item_tuples):\n",
    "    return [title for (_, title) in item_tuples]\n",
    "\n",
    "# Function to compute BLEU score between two lists of titles\n",
    "def compute_bleu_score(reference, candidate):\n",
    "    reference_tokens = [word_tokenize(str(title)) for title in reference]\n",
    "    candidate_tokens = word_tokenize(str(candidate[0]))  # Ensure candidate is a single tokenized sentence\n",
    "    \n",
    "    # Compute BLEU score\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    return sentence_bleu(reference_tokens, candidate_tokens, smoothing_function=smoothing)\n",
    "# Compute BLEU scores for each row\n",
    "clicked_data_user_history['bleu_rl_vs_presented'] = clicked_data_user_history.apply(\n",
    "    lambda row: compute_bleu_score(\n",
    "        extract_titles(get_item_ids_and_titles(row['presented_slate'], news_df)),\n",
    "        extract_titles(get_item_ids_and_titles(row['slateq_slates'], news_df)),  # Replace None with your news_df# Replace None with your news_df\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "clicked_data_user_history['bleu_llm_vs_presented'] = clicked_data_user_history.apply(\n",
    "    lambda row: compute_bleu_score(\n",
    "        extract_titles(get_item_ids_and_titles(row['presented_slate'], news_df)) ,\n",
    "        extract_titles(get_item_ids_and_titles(row['llm_slateq_slate'], news_df)),  # Replace None with your news_df# Replace None with your news_df\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "clicked_data_user_history['bleu_reranked_vs_presented'] = clicked_data_user_history.apply(\n",
    "    lambda row: compute_bleu_score(\n",
    "        extract_titles(get_item_ids_and_titles(row['presented_slate'], news_df)) ,\n",
    "        extract_titles(get_item_ids_and_titles(row['slateq_reranked'], news_df)),  # Replace None with your news_df# Replace None with your news_df\n",
    "    ), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group-level averages:\n",
      "                             initial_user_state_tuple  \\\n",
      "0   (0.13400940597057343, 0.2015896886587143, -0.0...   \n",
      "1   (0.15157544612884521, 0.33397215604782104, -0....   \n",
      "2   (0.15727820992469788, 0.2477571815252304, -0.1...   \n",
      "3   (0.1577419489622116, 0.21108318865299225, -0.0...   \n",
      "4   (0.15791304409503937, 0.29125070571899414, -0....   \n",
      "..                                                ...   \n",
      "95  (0.2903972268104553, 0.15165531635284424, -0.0...   \n",
      "96  (0.2935115098953247, 0.13266007602214813, -0.1...   \n",
      "97  (0.29664674401283264, 0.14605002105236053, 0.0...   \n",
      "98  (0.29784202575683594, 0.2651831805706024, 0.01...   \n",
      "99  (0.2985374331474304, 0.1253967434167862, 0.048...   \n",
      "\n",
      "    group_mean_bleu_rl_vs_presented  \n",
      "0                          0.016792  \n",
      "1                          0.021105  \n",
      "2                          0.500000  \n",
      "3                          1.000000  \n",
      "4                          0.021016  \n",
      "..                              ...  \n",
      "95                         0.289239  \n",
      "96                         1.000000  \n",
      "97                         0.519756  \n",
      "98                         0.018477  \n",
      "99                         0.511115  \n",
      "\n",
      "[100 rows x 2 columns]\n",
      "\n",
      "Overall average:\n",
      "0.3701003809647881\n"
     ]
    }
   ],
   "source": [
    "grouped_means_bleu_rl = clicked_data_user_history.groupby('initial_user_state_tuple')['bleu_rl_vs_presented'].mean().reset_index()\n",
    "grouped_means_bleu_rl.rename(columns={'bleu_rl_vs_presented': 'group_mean_bleu_rl_vs_presented'}, inplace=True)\n",
    "\n",
    "# Step 3: Calculate the overall average of the group means\n",
    "overall_mean = grouped_means_bleu_rl['group_mean_bleu_rl_vs_presented'].mean()\n",
    "\n",
    "# Display the results\n",
    "print(\"Group-level averages:\")\n",
    "print(grouped_means_bleu_rl)\n",
    "print(\"\\nOverall average:\")\n",
    "print(overall_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group-level averages:\n",
      "                             initial_user_state_tuple  \\\n",
      "0   (0.13400940597057343, 0.2015896886587143, -0.0...   \n",
      "1   (0.15157544612884521, 0.33397215604782104, -0....   \n",
      "2   (0.15727820992469788, 0.2477571815252304, -0.1...   \n",
      "3   (0.1577419489622116, 0.21108318865299225, -0.0...   \n",
      "4   (0.15791304409503937, 0.29125070571899414, -0....   \n",
      "..                                                ...   \n",
      "95  (0.2903972268104553, 0.15165531635284424, -0.0...   \n",
      "96  (0.2935115098953247, 0.13266007602214813, -0.1...   \n",
      "97  (0.29664674401283264, 0.14605002105236053, 0.0...   \n",
      "98  (0.29784202575683594, 0.2651831805706024, 0.01...   \n",
      "99  (0.2985374331474304, 0.1253967434167862, 0.048...   \n",
      "\n",
      "    group_mean_bleu_reranked_vs_presented  \n",
      "0                                0.509399  \n",
      "1                                0.681157  \n",
      "2                                0.025915  \n",
      "3                                0.021359  \n",
      "4                                0.017476  \n",
      "..                                    ...  \n",
      "95                               0.511045  \n",
      "96                               1.000000  \n",
      "97                               0.756944  \n",
      "98                               0.015966  \n",
      "99                               0.682333  \n",
      "\n",
      "[100 rows x 2 columns]\n",
      "\n",
      "Overall average:\n",
      "0.40505892939061766\n"
     ]
    }
   ],
   "source": [
    "grouped_means_bleu_bm25 = clicked_data_user_history.groupby('initial_user_state_tuple')['bleu_reranked_vs_presented'].mean().reset_index()\n",
    "grouped_means_bleu_bm25.rename(columns={'bleu_reranked_vs_presented': 'group_mean_bleu_reranked_vs_presented'}, inplace=True)\n",
    "\n",
    "# Step 3: Calculate the overall average of the group means\n",
    "overall_mean = grouped_means_bleu_bm25['group_mean_bleu_reranked_vs_presented'].mean()\n",
    "\n",
    "# Display the results\n",
    "print(\"Group-level averages:\")\n",
    "print(grouped_means_bleu_bm25)\n",
    "print(\"\\nOverall average:\")\n",
    "print(overall_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group-level averages:\n",
      "                             initial_user_state_tuple  \\\n",
      "0   (0.13400940597057343, 0.2015896886587143, -0.0...   \n",
      "1   (0.15157544612884521, 0.33397215604782104, -0....   \n",
      "2   (0.15727820992469788, 0.2477571815252304, -0.1...   \n",
      "3   (0.1577419489622116, 0.21108318865299225, -0.0...   \n",
      "4   (0.15791304409503937, 0.29125070571899414, -0....   \n",
      "..                                                ...   \n",
      "95  (0.2903972268104553, 0.15165531635284424, -0.0...   \n",
      "96  (0.2935115098953247, 0.13266007602214813, -0.1...   \n",
      "97  (0.29664674401283264, 0.14605002105236053, 0.0...   \n",
      "98  (0.29784202575683594, 0.2651831805706024, 0.01...   \n",
      "99  (0.2985374331474304, 0.1253967434167862, 0.048...   \n",
      "\n",
      "    group_mean_bleu_llm_vs_presented  \n",
      "0                           0.022188  \n",
      "1                           0.672187  \n",
      "2                           0.026315  \n",
      "3                           1.000000  \n",
      "4                           0.021973  \n",
      "..                               ...  \n",
      "95                          0.518924  \n",
      "96                          1.000000  \n",
      "97                          1.000000  \n",
      "98                          0.038854  \n",
      "99                          0.198119  \n",
      "\n",
      "[100 rows x 2 columns]\n",
      "\n",
      "Overall average:\n",
      "0.46988566939943555\n"
     ]
    }
   ],
   "source": [
    "grouped_means_bleu_llm = clicked_data_user_history.groupby('initial_user_state_tuple')['bleu_llm_vs_presented'].mean().reset_index()\n",
    "grouped_means_bleu_llm.rename(columns={'bleu_llm_vs_presented': 'group_mean_bleu_llm_vs_presented'}, inplace=True)\n",
    "\n",
    "# Step 3: Calculate the overall average of the group means\n",
    "overall_mean = grouped_means_bleu_llm['group_mean_bleu_llm_vs_presented'].mean()\n",
    "\n",
    "# Display the results\n",
    "print(\"Group-level averages:\")\n",
    "print(grouped_means_bleu_llm)\n",
    "print(\"\\nOverall average:\")\n",
    "print(overall_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "feather_file_path_llm= gen_slates_dir / \"llm_slates.feather\"\n",
    "df_llm = pd.read_feather(feather_file_path_llm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group-level averages:\n",
      "                             initial_user_state_tuple  group_mean_hit\n",
      "0   (-0.0259791798889637, 0.5851805210113525, 0.00...        0.500000\n",
      "1   (0.049241334199905396, 0.23491184413433075, 0....        0.333333\n",
      "2   (0.06917192041873932, 0.22154483199119568, 0.0...        0.000000\n",
      "3   (0.0808219462633133, 0.280564546585083, -0.035...        0.142857\n",
      "4   (0.11326862126588821, 0.38108861446380615, -0....        0.285714\n",
      "..                                                ...             ...\n",
      "95  (0.3616601228713989, 0.1373804360628128, 0.021...        0.000000\n",
      "96  (0.3617928624153137, 0.11457429826259613, 0.01...        0.000000\n",
      "97  (0.36578845977783203, 0.042487733066082, 0.081...        0.500000\n",
      "98  (0.39353927969932556, 0.0698390007019043, -0.0...        0.000000\n",
      "99  (0.3967207372188568, 0.18588663637638092, -0.0...        0.000000\n",
      "\n",
      "[100 rows x 2 columns]\n",
      "\n",
      "Overall average:\n",
      "0.16706782106782106\n"
     ]
    }
   ],
   "source": [
    "df_llm= df_llm[df_llm['llm_gen_slate'].apply(lambda x: len(x) > 0)].copy()\n",
    "df_llm['initial_user_state_tuple'] = df_llm['initial_user_state'].apply(tuple)\n",
    "\n",
    "# Step 2: Group by initial_user_state and calculate the mean of 'hit' for each group\n",
    "grouped_means_rl_llm = df_llm.groupby('initial_user_state_tuple')['llm_hit'].mean().reset_index()\n",
    "grouped_means_rl_llm.rename(columns={'llm_hit': 'group_mean_hit'}, inplace=True)\n",
    "\n",
    "# Step 3: Calculate the overall average of the group means\n",
    "overall_mean = grouped_means_rl_llm['group_mean_hit'].mean()\n",
    "\n",
    "# Display the results\n",
    "print(\"Group-level averages:\")\n",
    "print(grouped_means_rl_llm)\n",
    "print(\"\\nOverall average:\")\n",
    "print(overall_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average S-Recall for llm_slate:\n",
      "Category Level: 0.3645, Subcategory Level: 0.6131\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary for quick lookup of category and subcategory\n",
    "item_to_category = dict(zip(news_df['itemId'], news_df['category']))\n",
    "item_to_subcategory = dict(zip(news_df['itemId'], news_df['subcategory']))\n",
    "\n",
    "# Calculate total unique categories and subcategories in the dataset\n",
    "total_categories = news_df['category'].nunique()\n",
    "total_subcategories = news_df['subcategory'].nunique()\n",
    "\n",
    "# Function to calculate diversity metrics for a given slate\n",
    "def calculate_diversity(slate, item_to_category, item_to_subcategory):\n",
    "    categories = set()\n",
    "    subcategories = set()\n",
    "    \n",
    "    for item in slate:\n",
    "        if item in item_to_category:\n",
    "            categories.add(item_to_category[item])\n",
    "        if item in item_to_subcategory:\n",
    "            subcategories.add(item_to_subcategory[item])\n",
    "    \n",
    "    return len(categories), len(subcategories)\n",
    "\n",
    "# Function to calculate S-Recall as a ratio\n",
    "def calculate_s_recall(df, column, item_to_category, item_to_subcategory, total_categories, total_subcategories):\n",
    "    results = []\n",
    "    \n",
    "    for slate in df[column]:\n",
    "        category_diversity, subcategory_diversity = calculate_diversity(slate, item_to_category, item_to_subcategory)\n",
    "        \n",
    "        # Calculate S-Recall as a ratio\n",
    "        s_recall_category = category_diversity / len(slate)\n",
    "        s_recall_subcategory = subcategory_diversity / len(slate)\n",
    "        \n",
    "        results.append((s_recall_category, s_recall_subcategory))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Calculate S-Recall for each slate column\n",
    "llm_s_recall = calculate_s_recall(df_llm, 'llm_gen_slate', item_to_category, item_to_subcategory, total_categories, total_subcategories)\n",
    "\n",
    "# Calculate average S-Recall for each column\n",
    "def calculate_average_s_recall(s_recall_results):\n",
    "    avg_category = sum([x[0] for x in s_recall_results]) / len(s_recall_results)\n",
    "    avg_subcategory = sum([x[1] for x in s_recall_results]) / len(s_recall_results)\n",
    "    return avg_category, avg_subcategory\n",
    "\n",
    "llm_avg_category, llm_avg_subcategory = calculate_average_s_recall(llm_s_recall)\n",
    "\n",
    "\n",
    "# Print average S-Recall for each column\n",
    "print(\"Average S-Recall for llm_slate:\")\n",
    "print(f\"Category Level: {llm_avg_category:.4f}, Subcategory Level: {llm_avg_subcategory:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average S-Recall for llm_slate:\n",
      "Category Level: 0.3645, Subcategory Level: 0.0676\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a dictionary to map categories to their subcategories\n",
    "category_to_subcategories = news_df.groupby('category')['subcategory'].unique().to_dict()\n",
    "\n",
    "# Function to calculate diversity metrics for a given slate\n",
    "def calculate_diversity(slate, item_to_category, item_to_subcategory):\n",
    "    categories = set()\n",
    "    subcategories = set()\n",
    "    \n",
    "    for item in slate:\n",
    "        if item in item_to_category:\n",
    "            categories.add(item_to_category[item])\n",
    "        if item in item_to_subcategory:\n",
    "            subcategories.add(item_to_subcategory[item])\n",
    "    \n",
    "    return categories, subcategories\n",
    "\n",
    "# Function to calculate S-Recall as a ratio\n",
    "def calculate_s_recall(df, column, item_to_category, item_to_subcategory, category_to_subcategories):\n",
    "    results = []\n",
    "    \n",
    "    for slate in df[column]:\n",
    "        categories_in_slate, subcategories_in_slate = calculate_diversity(slate, item_to_category, item_to_subcategory)\n",
    "        \n",
    "        # Calculate S-Recall at category level\n",
    "        s_recall_category = len(categories_in_slate) / len(slate)\n",
    "        \n",
    "        # Calculate S-Recall at subcategory level (contextual to categories in the slate)\n",
    "        total_subcategories_in_categories = set()\n",
    "        for category in categories_in_slate:\n",
    "            total_subcategories_in_categories.update(category_to_subcategories[category])\n",
    "        \n",
    "        s_recall_subcategory = len(subcategories_in_slate) / len(total_subcategories_in_categories)\n",
    "        \n",
    "        results.append((s_recall_category, s_recall_subcategory))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Calculate S-Recall for each slate column\n",
    "llm_s_recall = calculate_s_recall(df_llm, 'llm_gen_slate', item_to_category, item_to_subcategory, category_to_subcategories)\n",
    "\n",
    "# Calculate average S-Recall for each column\n",
    "def calculate_average_s_recall(s_recall_results):\n",
    "    avg_category = sum([x[0] for x in s_recall_results]) / len(s_recall_results)\n",
    "    avg_subcategory = sum([x[1] for x in s_recall_results]) / len(s_recall_results)\n",
    "    return avg_category, avg_subcategory\n",
    "\n",
    "llm_avg_category, llm_avg_subcategory = calculate_average_s_recall(llm_s_recall)\n",
    "rl_avg_category, rl_avg_subcategory = calculate_average_s_recall(rl_s_recall)\n",
    "slate_avg_category, slate_avg_subcategory = calculate_average_s_recall(slate_reranked_recall)\n",
    "\n",
    "# Print average S-Recall for each column\n",
    "print(\"Average S-Recall for llm_slate:\")\n",
    "print(f\"Category Level: {llm_avg_category:.4f}, Subcategory Level: {llm_avg_subcategory:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicked_data_user_history = category_data.merge(\n",
    "    df_llm,\n",
    "    left_on=['click', 'observed_state'],\n",
    "    right_on=['original_click', 'initial_user_state_tuple'],\n",
    "    how='right'  # Use 'inner' to keep only matching rows\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract titles from the list of tuples\n",
    "def extract_titles(item_tuples):\n",
    "    return [title for (_, title) in item_tuples]\n",
    "\n",
    "# Function to compute BLEU score between two lists of titles\n",
    "def compute_bleu_score(reference, candidate):\n",
    "    reference_tokens = [word_tokenize(str(title)) for title in reference]\n",
    "    candidate_tokens = word_tokenize(str(candidate[0]))  # Ensure candidate is a single tokenized sentence\n",
    "    \n",
    "    # Compute BLEU score\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    return sentence_bleu(reference_tokens, candidate_tokens, smoothing_function=smoothing)\n",
    "\n",
    "\n",
    "clicked_data_user_history['bleu_llm_vs_presented'] = clicked_data_user_history.apply(\n",
    "    lambda row: compute_bleu_score(\n",
    "        extract_titles(get_item_ids_and_titles(row['presented_slate'], news_df)) ,\n",
    "        extract_titles(get_item_ids_and_titles(row['llm_gen_slate'], news_df)),  # Replace None with your news_df# Replace None with your news_df\n",
    "    ), axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group-level averages:\n",
      "                             initial_user_state_tuple  \\\n",
      "0   (-0.0259791798889637, 0.5851805210113525, 0.00...   \n",
      "1   (0.049241334199905396, 0.23491184413433075, 0....   \n",
      "2   (0.06917192041873932, 0.22154483199119568, 0.0...   \n",
      "3   (0.0808219462633133, 0.280564546585083, -0.035...   \n",
      "4   (0.11326862126588821, 0.38108861446380615, -0....   \n",
      "..                                                ...   \n",
      "95  (0.3616601228713989, 0.1373804360628128, 0.021...   \n",
      "96  (0.3617928624153137, 0.11457429826259613, 0.01...   \n",
      "97  (0.36578845977783203, 0.042487733066082, 0.081...   \n",
      "98  (0.39353927969932556, 0.0698390007019043, -0.0...   \n",
      "99  (0.3967207372188568, 0.18588663637638092, -0.0...   \n",
      "\n",
      "    group_mean_bleu_llm_vs_presented  \n",
      "0                           1.000000  \n",
      "1                           0.021973  \n",
      "2                           0.018409  \n",
      "3                           0.041316  \n",
      "4                           0.585378  \n",
      "..                               ...  \n",
      "95                          0.018477  \n",
      "96                          0.043242  \n",
      "97                          0.213435  \n",
      "98                          0.029069  \n",
      "99                          1.000000  \n",
      "\n",
      "[100 rows x 2 columns]\n",
      "\n",
      "Overall average:\n",
      "0.47951284871133226\n"
     ]
    }
   ],
   "source": [
    "grouped_means_bleu_llm = clicked_data_user_history.groupby('initial_user_state_tuple')['bleu_llm_vs_presented'].mean().reset_index()\n",
    "grouped_means_bleu_llm.rename(columns={'bleu_llm_vs_presented': 'group_mean_bleu_llm_vs_presented'}, inplace=True)\n",
    "\n",
    "# Step 3: Calculate the overall average of the group means\n",
    "overall_mean = grouped_means_bleu_llm['group_mean_bleu_llm_vs_presented'].mean()\n",
    "\n",
    "# Display the results\n",
    "print(\"Group-level averages:\")\n",
    "print(grouped_means_bleu_llm)\n",
    "print(\"\\nOverall average:\")\n",
    "print(overall_mean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
